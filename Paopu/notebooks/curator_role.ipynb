{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curator Role\n",
    "\n",
    "In this notebook we put everything together to create the *curator role* of the Jetbot Robot. The main functionalities are the *Paopu* detector, to detect if a player wins, the person counter/detector and the motion detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from jetbot import Camera\n",
    "from jetbot import bgr8_to_jpeg\n",
    "from jetbot import ObjectDetector\n",
    "from jetbot import Robot\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import simpleaudio as sa\n",
    "import time\n",
    "import traitlets\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we define all the functions needed for the *Paopu* detection functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"en\"    # ca\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value, device):\n",
    "    \"\"\"\n",
    "    Preprocess image for paopu detector model.\n",
    "    \n",
    "    Arguments:\n",
    "        camera_value (array): Image from Jetbot Camera.\n",
    "        device (torch.device): cpu or cuda\n",
    "    \"\"\" \n",
    "    global normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def play_sound(filename, sync=False):\n",
    "    \"\"\"\n",
    "    Play a wav sound file.\n",
    "    \"\"\"\n",
    "    wave_obj = sa.WaveObject.from_wave_file(filename)\n",
    "    \n",
    "    play_obj = wave_obj.play()\n",
    "    if sync:\n",
    "        play_obj.wait_done()\n",
    "        \n",
    "    return play_obj\n",
    "\n",
    "\n",
    "def lose_sequence():\n",
    "    play_obj = play_sound(f\"../Music/{LANG}/gameover.wav\")\n",
    "    robot.left(0.2)\n",
    "    play_obj.wait_done()\n",
    "    robot.stop()\n",
    "    \n",
    "    \n",
    "def win_sequence():\n",
    "    play_obj = play_sound(f\"../Music/{LANG}/winsound.wav\")\n",
    "    robot.left(0.2)\n",
    "    play_obj.wait_done()\n",
    "    play_sound(f\"../Music/{LANG}/win.wav\")\n",
    "    robot.stop()\n",
    "\n",
    "    \n",
    "def detect_hand(model):\n",
    "    x = preprocess(camera.value, device)\n",
    "    y = model(x)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_hand = float(y.flatten()[1])\n",
    "    hand_slider.value = prob_hand\n",
    "    \n",
    "    if prob_hand > 0.9:\n",
    "        #time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def curator_role(robot, camera, paopu_detector, person_detector, device, image_widget, video_writer=None, total_players=3, movement_threshold=40):\n",
    "    \n",
    "    # Total players list\n",
    "    players_list = [i+1 for i in range(total_players)]\n",
    "    \n",
    "    print(\"Starting curator role...\")\n",
    "    \n",
    "    play = True\n",
    "    \n",
    "    while play:\n",
    "        robot.stop()\n",
    "        x = preprocess(camera.value, device)\n",
    "        y = paopu_detector(x)\n",
    "        y = F.softmax(y, dim=1)\n",
    "\n",
    "        prob_pao = float(y.flatten()[3])\n",
    "        prob_move = float(y.flatten()[2])\n",
    "        prob_hand = float(y.flatten()[1])\n",
    "        prob_environment = float(y.flatten()[0])\n",
    "        \n",
    "        pao_slider.value = prob_pao\n",
    "        move_slider.value = prob_move\n",
    "        hand_slider.value = prob_hand\n",
    "        environment_slider.value = prob_environment\n",
    "\n",
    "        if prob_pao > 0.94:# and prob_move < 0.1:\n",
    "            \n",
    "            # Start green melody process\n",
    "            greenlight_play_obj = play_sound(f\"../Music/{LANG}/greenlight.wav\")\n",
    "\n",
    "            # Check for hand during green state ()               \n",
    "            t0 = time.time()\n",
    "            elapsed_time = 0\n",
    "            while elapsed_time < 4:\n",
    "                win = detect_hand(paopu_detector)\n",
    "                if win:\n",
    "                    break\n",
    "                elapsed_time = time.time() - t0\n",
    "            \n",
    "            # If player wins, end game\n",
    "            if win:\n",
    "                greenlight_play_obj.stop()\n",
    "                win_sequence()\n",
    "                print(\"You won!\\nEnding game...\")\n",
    "                play = False    #break\n",
    "            \n",
    "            # Else, move 180º and detect movement\n",
    "            else:\n",
    "                robot.right(0.15)\n",
    "                time.sleep(1.15)   # Move right time\n",
    "                robot.stop()\n",
    "                time.sleep(1.0)\n",
    "                \n",
    "                # Detect movement\n",
    "                print('Detecting movement...')\n",
    "                detections = detect_person_motion(camera, person_detector, image_widget, 3, video_writer=video_writer, threshold=movement_threshold)\n",
    "                \n",
    "                if len(detections):                   \n",
    "                    str_nums = \", \".join([str(i) for i in detections])\n",
    "                    print(f'Person(s) moving: {str_nums}')\n",
    "                    \n",
    "                    # Play sound for each of the players eliminated\n",
    "                    for n in detections:\n",
    "                        print(f'\\tPlayer {players_list[n]} eliminated!')\n",
    "                        play_sound(f\"../Music/{LANG}/player.wav\", True)\n",
    "                        play_sound(f\"../Music/{LANG}/{n}.wav\", True)\n",
    "                        play_sound(f\"../Music/{LANG}/eliminated.wav\", True)\n",
    "                    \n",
    "                    for n in detections:\n",
    "                        players_list.remove(players_list[n-1])\n",
    "                                            \n",
    "                    if not len(players_list):\n",
    "                        # End game when all players eliminated\n",
    "                        print(\"GAME OVER\")\n",
    "                        lose_sequence()\n",
    "                        play = False\n",
    "                        break\n",
    "                    \n",
    "                # Go back to wall\n",
    "                robot.right(0.15)\n",
    "                time.sleep(1.10)\n",
    "                robot.stop()\n",
    "                    \n",
    "                #T = T * 1.1 # increase threshold as players move closer\n",
    "                    \n",
    "        # TODO: what happens when player touches wall while robot is still moving (outside green melody)?\n",
    "            \n",
    "        else:\n",
    "            # Move right in order to find Paopu image\n",
    "            robot.right(0.12)\n",
    "            \n",
    "    # Stop robot when game is finished\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define all the functions needed for the person motion detection functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for person motion detection\n",
    "\n",
    "def detect_people(object_detector, image, conf_thr):\n",
    "    \"\"\"\n",
    "    Detect people on an image and return bounding boxes.\n",
    "\n",
    "    Arguments:\n",
    "        object_detector: ObjectDetector model\n",
    "        image (array): input image\n",
    "        conf_thr (float): confidence threshold\n",
    "        \n",
    "    Returns:\n",
    "            list: of bounding boxes (left, top, right, bottom)\n",
    "    \"\"\"\n",
    "\n",
    "    person_class = 1\n",
    "\n",
    "    # Image size\n",
    "    rows = 300\n",
    "    cols = 300\n",
    "\n",
    "    # Make prediction on image\n",
    "    detections = object_detector(image)\n",
    "\n",
    "    # Iterate over each detection and save boundig box \n",
    "    # if confidence is above threshold and detected class is person\n",
    "    person_boxes = []\n",
    "\n",
    "    for detection in detections[0]:\n",
    "            if detection['confidence'] > conf_thr and detection['label'] == person_class:\n",
    "                    left = int(detection['bbox'][0] * cols)\n",
    "                    top = int(detection['bbox'][1]  * rows)\n",
    "                    right = int(detection['bbox'][2] * cols)\n",
    "                    bottom = int(detection['bbox'][3]  * rows)\n",
    "                    person_boxes.append([left, top, right, bottom])\n",
    "\n",
    "    return sorted(person_boxes)\n",
    "\n",
    "# Generate a random color palette\n",
    "COLORS = np.random.uniform(0, 255, size=(15, 3))\n",
    "\n",
    "\n",
    "def plot_boxes(image, people_boxes, motion_boxes=[]):\n",
    "    \"\"\"\n",
    "    Plot bounding boxes on an image.\n",
    "\n",
    "    Arguments:\n",
    "        image (array): input image\n",
    "        people_boxes (array): array of people bounding boxes with [left, top, right, bottom] positions\n",
    "        motion_boxes (array): array of motion bounding boxes with [left, top, right, bottom] positions\n",
    "    \"\"\"\n",
    "    # Plot people bounding boxes and corresponding number\n",
    "    for i in range(len(people_boxes)):\n",
    "        bbox = people_boxes[i]\n",
    "\n",
    "        left = bbox[0]\n",
    "        top = bbox[1]\n",
    "        right = bbox[2]\n",
    "        bottom = bbox[3]\n",
    "\n",
    "        # Plot bounding box\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), COLORS[i], thickness=2)\n",
    "        cv2.putText(image, f'{i+1}', (int(left)+5, int(top)+20), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Plot motion bounding boxes in green\n",
    "    for bbox in motion_boxes:\n",
    "        left = bbox[0]\n",
    "        top = bbox[1]\n",
    "        right = bbox[2]\n",
    "        bottom = bbox[3]\n",
    "\n",
    "        # Plot bounding box\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), thickness=1)\n",
    "        \n",
    "    return image\n",
    " \n",
    "    \n",
    "def display_image(image, image_widget, text1=\"\", text2=\"\"):\n",
    "    \"\"\"\n",
    "    Display an image on a Jupyter Widget and optionally puts top/bottom text.\n",
    "    \n",
    "    Arguments:\n",
    "        image (array): image to display.\n",
    "        img_widget (widgets.Image, optional): widget used to display the image\n",
    "        text1 (str, optional): Optional 1st text to print on the top of image.\n",
    "        text2 (str, optional): Optional 2nd text to print on the top of image.\n",
    "    \"\"\"\n",
    "                \n",
    "    # Add optional text\n",
    "    if text1:\n",
    "        cv2.putText(image, text1, (10, 20), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    if text2:\n",
    "        cv2.putText(image, text2, (10, 40), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "        \n",
    "    # Display image\n",
    "    image_jpeg = bgr8_to_jpeg(image)\n",
    "    image_widget.value = image_jpeg\n",
    "\n",
    "\n",
    "def motion_detection(firstFrame, newFrame, threshold=40):\n",
    "    \"\"\"\n",
    "    Detect motion by comparing the newFrame with a firstFrame.\n",
    "    \n",
    "    Arguments:\n",
    "        firstFrame (array): first frame considered as the baseline.\n",
    "        newFrame (array): new frame.\n",
    "        threshold (int): movement threshold.\n",
    "        \n",
    "    Returns:\n",
    "        list: of regions where movement has been detected (left, top, right, bottom)\n",
    "    \"\"\"\n",
    "    # Adapted from https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/\n",
    "\n",
    "    # Min movement area\n",
    "    MIN_AREA = 50\n",
    "\n",
    "    # Gaussian blur kernel\n",
    "    KERNEL_SIZE = 3\n",
    "\n",
    "    # List to store regions where movement is detected\n",
    "    motion_bboxes = []\n",
    "    \n",
    "    # firstFrame preprocessing\n",
    "    firstFrame = cv2.cvtColor(firstFrame, cv2.COLOR_BGR2GRAY)\n",
    "    firstFrame = cv2.GaussianBlur(firstFrame, (KERNEL_SIZE, KERNEL_SIZE), 0)\n",
    "    \n",
    "    # grab the current frame and initialize the static/moving text\n",
    "    frame = newFrame.copy()\n",
    "\n",
    "    # convert frame to grayscale, and blur it\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (KERNEL_SIZE, KERNEL_SIZE), 0)\n",
    "\n",
    "    # compute the absolute difference between the current frame and\n",
    "    # first frame\n",
    "    frameDelta = cv2.absdiff(firstFrame, gray)\n",
    "    thresh = cv2.threshold(frameDelta, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # dilate the thresholded image to fill in holes, then find contours\n",
    "    # on thresholded image\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # if the contour is too small, ignore it\n",
    "        if cv2.contourArea(c) < MIN_AREA:\n",
    "            continue\n",
    "\n",
    "        # compute the bounding box for the contour, draw it on the frame,\n",
    "        # and update the text\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        \n",
    "        motion_bboxes.append((x, y, x+w, y+h))    # (left, top, right, bottom)\n",
    "    \n",
    "    return motion_bboxes\n",
    "\n",
    "def box_in_box(in_box, out_box, margin=0):\n",
    "    \"\"\"\n",
    "    Evaluate if a bounding box is partially inside another one.\n",
    "    \n",
    "    Arguments:\n",
    "        in_box (array): inner bounding box (left, top, right, bottom).\n",
    "        out_box (array): outter bounding box (left, top, right, bottom).\n",
    "        margin (int, optional): percentual margin to add to outter box. Defaults to 0.\n",
    "        \n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    top_left = False\n",
    "    bottom_right = False\n",
    "    \n",
    "    # Apply margin\n",
    "    out_box[0] = int((1 - margin/100) * out_box[0])\n",
    "    out_box[1] = int((1 - margin/100) * out_box[1])\n",
    "    out_box[2] = int((1 + margin/100) * out_box[2])\n",
    "    out_box[3] = int((1 + margin/100) * out_box[3])\n",
    "    \n",
    "    # Top-left corner\n",
    "    if out_box[0] <= in_box[0] <= out_box[2] and out_box[1] <= in_box[1] <= out_box[3]:\n",
    "        top_left = True\n",
    "        \n",
    "    # Bottom-right corner\n",
    "    if out_box[0] <= in_box[2] <= out_box[2] and out_box[1] <= in_box[3] <= out_box[3]:\n",
    "        bottom_right = True\n",
    "            \n",
    "    # Return True if top-left and bottom-right corners are inside\n",
    "    if top_left and bottom_right:\n",
    "        return True\n",
    "    \n",
    "def allow_movement(jetbot_camera, object_detector, image_widget, detection_time, video_writer=None):\n",
    "    \"\"\"\n",
    "    Perform person detection and display results.\n",
    "    \n",
    "    Arguments:\n",
    "        jetbot_camera(Camera): Jetbot camera initialized with size (300, 300).\n",
    "        object_detector: ObjectDetector model\n",
    "        image_widget (widgets.Image): Jupyter image widget.\n",
    "        video_writer (cv2.VideoWriter, optional): OpenCV video writer used to save output.\n",
    "        \n",
    "    \"\"\"\n",
    "    initial_t = time.time()\n",
    "    elapsed_time = 0\n",
    "\n",
    "    while elapsed_time < detection_time:\n",
    "        # Make people detections\n",
    "        img = jetbot_camera.value.copy()\n",
    "        people_detections = detect_people(object_detector, img, 0.1)\n",
    "        \n",
    "        # Display image and detections\n",
    "        image = plot_boxes(img, people_detections)\n",
    "        display_image(image, image_widget, 'MOVE', f'{int(detection_time - elapsed_time)}s')\n",
    "        \n",
    "        # Save video\n",
    "        if video_writer:\n",
    "            video_writer.write(image)\n",
    "            \n",
    "        elapsed_time =  time.time() - initial_t\n",
    "\n",
    "    \n",
    "def detect_person_motion(jetbot_camera, object_detector, image_widget, max_time, threshold=40, video_writer=None):\n",
    "    \"\"\"\n",
    "    Perform person and motion detection in a detection window of max time\n",
    "    and display results.\n",
    "    \n",
    "    Arguments:\n",
    "        jetbot_camera(Camera): Jetbot camera initialized with size (300, 300).\n",
    "        max_time (int): Detection window time.\n",
    "        image_widget (widgets.Image): Jupyter image widget.\n",
    "        running_process(process): Running process during which the detection is performed.\n",
    "        threshold (int): movement threshold.\n",
    "        video_writer (cv2.VideoWriter, optional): OpenCV video writer used to save output.\n",
    "    \"\"\"    \n",
    "    # Get first frame\n",
    "    first_frame = jetbot_camera.value.copy()\n",
    "\n",
    "    initial_t = time.time()\n",
    "    elapsed_time = 0\n",
    "    t = \"\"\n",
    "\n",
    "    # Make people detections\n",
    "    people_detections = detect_people(object_detector, jetbot_camera.value.copy(), 0.1)\n",
    "    \n",
    "    # Run motion detection while process is running\n",
    "    moving_persons_total = []\n",
    "    \n",
    "    while elapsed_time < max_time:\n",
    "        # Get new frame\n",
    "        img = jetbot_camera.value.copy()\n",
    "        \n",
    "        # Detect motion\n",
    "        m_bboxes = motion_detection(first_frame, img, threshold)        \n",
    "        m_bboxes = set(m_bboxes)    # remove duplicates\n",
    "        \n",
    "        # Detect which person is moving\n",
    "        moving_persons = []\n",
    "        for i in range(len(people_detections)):\n",
    "            out_box = people_detections[i]                \n",
    "            person_num = i+1\n",
    "            \n",
    "            for in_box in m_bboxes:\n",
    "                if box_in_box(list(in_box), out_box):\n",
    "                    if person_num not in moving_persons:\n",
    "                        moving_persons.append(person_num)\n",
    "                        \n",
    "        moving_persons_total.extend(moving_persons)\n",
    "        \n",
    "        # Persons moved in this frame\n",
    "        if len(moving_persons):\n",
    "            str_nums = \", \".join([str(i) for i in moving_persons])\n",
    "            t = f'Moving: {str_nums}'\n",
    "        \n",
    "        # Display image and detections\n",
    "        image = plot_boxes(img, people_detections, m_bboxes)\n",
    "        display_image(image, image_widget, 'STOP', t)\n",
    "        \n",
    "        # Save video\n",
    "        if video_writer:\n",
    "            video_writer.write(image)\n",
    "            \n",
    "        elapsed_time =  time.time() - initial_t\n",
    "\n",
    "    # Print persons moved in detection window\n",
    "    moving_persons_total = set(moving_persons_total)\n",
    "#     if len(moving_persons_total):\n",
    "#         str_nums = \", \".join([str(i) for i in moving_persons_total])\n",
    "#         print(f'Person(s) moving: {str_nums}')\n",
    "        \n",
    "    return moving_persons_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the previously defined functions to create the *curator role*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init camera with same resolution as model input (300x300)\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mobilenet-v2 pretrained on COCO\n",
    "obj_detector_model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Paopu detection model\n",
    "paopu_detector_model = torchvision.models.alexnet(pretrained=False)\n",
    "paopu_detector_model.classifier[6] = torch.nn.Linear(paopu_detector_model.classifier[6].in_features, 4)\n",
    "paopu_detector_model.load_state_dict(torch.load('../../../Models/best_model3.pth'))    # Load pretrained weights\n",
    "\n",
    "device = torch.device('cuda')\n",
    "paopu_detector_model = paopu_detector_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init robot\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929dd536c95e43028848358a959d1048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widgets\n",
    "camera_widget = widgets.Image(format='jpeg', width=400, height=400)\n",
    "image_widget = widgets.Image(format='jpeg', width=400, height=400)\n",
    "\n",
    "pao_slider = widgets.FloatSlider(description='pao', min=0.0, max=1.0, orientation='vertical')\n",
    "move_slider = widgets.FloatSlider(description='move', min=0.0, max=1.0, orientation='vertical')\n",
    "hand_slider = widgets.FloatSlider(description='hand', min=0.0, max=1.0, orientation='vertical')\n",
    "environment_slider = widgets.FloatSlider(description='environment', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([camera_widget, image_widget, pao_slider, move_slider, hand_slider, environment_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the main curator function ```curator_role``` with the desired parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting curator role...\n",
      "Detecting movement...\n",
      "Detecting movement...\n",
      "Person(s) moving: 1\n",
      "\tPlayer 2 eliminated!\n"
     ]
    }
   ],
   "source": [
    "# Main curator\n",
    "\n",
    "# Config parameters\n",
    "N=2\n",
    "MOVEMENT_THRESHOLD = 45\n",
    "VIDEO_WRITER = False\n",
    "\n",
    "# Video Writer\n",
    "if VIDEO_WRITER:\n",
    "    cv2_video_writer = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (300,300))\n",
    "else:\n",
    "    cv2_video_writer = None\n",
    "\n",
    "try:\n",
    "    curator_role(robot, camera, paopu_detector_model, obj_detector_model, device, image_widget, cv2_video_writer, N, movement_threshold=MOVEMENT_THRESHOLD)\n",
    "except KeyboardInterrupt:\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement and detection test\n",
    "The cells belows are used to perform some tests and can be used to determine the duration of the turn action and the value for the movement threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test paopu detector\n",
    "x = preprocess(camera.value.copy(), device)\n",
    "y = paopu_detector_model(x)\n",
    "y = F.softmax(y, dim=1)\n",
    "\n",
    "prob_pao = float(y.flatten()[3])\n",
    "prob_move = float(y.flatten()[2])\n",
    "prob_hand = float(y.flatten()[1])\n",
    "prob_environment = float(y.flatten()[0])\n",
    "\n",
    "pao_slider.value = prob_pao\n",
    "move_slider.value = prob_move\n",
    "hand_slider.value = prob_hand\n",
    "environment_slider.value = prob_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robot 180 turn\n",
    "robot.right(0.15)\n",
    "time.sleep(1.05)\n",
    "robot.stop()\n",
    "time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "# Test person movement detector\n",
    "time.sleep(1)\n",
    "\n",
    "MOVEMENT_THRESHOLD = 50\n",
    "\n",
    "stop_time = 3\n",
    "move_time = 6\n",
    "\n",
    "# Video Writer\n",
    "cv2_video_writer = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (300,300))\n",
    "# cv2_video_writer = None\n",
    "\n",
    "# Main loop\n",
    "print('Start')\n",
    "while True:\n",
    "    try:\n",
    "        # MOVE - Allow movement\n",
    "        allow_movement(camera, obj_detector_model, image_widget, move_time, cv2_video_writer)\n",
    "\n",
    "        # STOP - Detect movement\n",
    "        detect_person_motion(camera, obj_detector_model, image_widget, stop_time, video_writer=cv2_video_writer, threshold=MOVEMENT_THRESHOLD)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print('Finish')        \n",
    "cv2_video_writer.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
