{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Motion Detection\n",
    "\n",
    "In this notebook we combine our person detection/counter with the motion detection, so that we can detect which person is moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from jetbot import Camera\n",
    "from jetbot import bgr8_to_jpeg\n",
    "from jetbot import ObjectDetector\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "import imutils\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mobilenet-v2 pretrained on COCO\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init camera with same resolution as model input (300x300)\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def detect_people(object_detector, image, conf_thr):\n",
    "    \"\"\"\n",
    "    Detect people on an image and return bounding boxes.\n",
    "\n",
    "    Arguments:\n",
    "        net: ObjectDetector model\n",
    "        image (array): input image\n",
    "        conf_thr (float): confidence threshold\n",
    "        \n",
    "    Returns:\n",
    "            list: of bounding boxes (left, top, right, bottom)\n",
    "    \"\"\"\n",
    "\n",
    "    person_class = 1\n",
    "\n",
    "    # Image size\n",
    "    rows = 300\n",
    "    cols = 300\n",
    "\n",
    "    # Make prediction on image\n",
    "    detections = object_detector(image)\n",
    "\n",
    "    # Iterate over each detection and save boundig box \n",
    "    # if confidence is above threshold and detected class is person\n",
    "    person_boxes = []\n",
    "\n",
    "    for detection in detections[0]:\n",
    "            if detection['confidence'] > conf_thr and detection['label'] == person_class:\n",
    "                    left = int(detection['bbox'][0] * cols)\n",
    "                    top = int(detection['bbox'][1]  * rows)\n",
    "                    right = int(detection['bbox'][2] * cols)\n",
    "                    bottom = int(detection['bbox'][3]  * rows)\n",
    "                    person_boxes.append([left, top, right, bottom])\n",
    "\n",
    "    return sorted(person_boxes)\n",
    "\n",
    "# Generate a random color palette\n",
    "COLORS = np.random.uniform(0, 255, size=(15, 3))\n",
    "\n",
    "\n",
    "def plot_boxes(image, people_boxes, motion_boxes=[]):\n",
    "    \"\"\"\n",
    "    Plot bounding boxes on an image.\n",
    "\n",
    "    Arguments:\n",
    "        image (array): input image\n",
    "        people_boxes (array): array of people bounding boxes with [left, top, right, bottom] positions\n",
    "        motion_boxes (array): array of motion bounding boxes with [left, top, right, bottom] positions\n",
    "    \"\"\"\n",
    "    # Plot people bounding boxes and corresponding number\n",
    "    for i in range(len(people_boxes)):\n",
    "        bbox = people_boxes[i]\n",
    "\n",
    "        left = bbox[0]\n",
    "        top = bbox[1]\n",
    "        right = bbox[2]\n",
    "        bottom = bbox[3]\n",
    "\n",
    "        # Plot bounding box\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), COLORS[i], thickness=2)\n",
    "        cv2.putText(image, f'{i+1}', (int(left)+5, int(top)+20), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Plot motion bounding boxes in green\n",
    "    for bbox in motion_boxes:\n",
    "        left = bbox[0]\n",
    "        top = bbox[1]\n",
    "        right = bbox[2]\n",
    "        bottom = bbox[3]\n",
    "\n",
    "        # Plot bounding box\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), thickness=1)\n",
    "        \n",
    "    return image\n",
    " \n",
    "    \n",
    "def display_image(image, image_widget, text1=\"\", text2=\"\"):\n",
    "    \"\"\"\n",
    "    Display an image on a Jupyter Widget and optionally puts top/bottom text.\n",
    "    \n",
    "    Arguments:\n",
    "        image (array): image to display.\n",
    "        img_widget (widgets.Image, optional): widget used to display the image\n",
    "        text1 (str, optional): Optional 1st text to print on the top of image.\n",
    "        text2 (str, optional): Optional 2nd text to print on the top of image.\n",
    "    \"\"\"\n",
    "                \n",
    "    # Add optional text\n",
    "    if text1:\n",
    "        cv2.putText(image, text1, (10, 20), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    if text2:\n",
    "        cv2.putText(image, text2, (10, 40), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "        \n",
    "    # Display image\n",
    "    image_jpeg = bgr8_to_jpeg(image)\n",
    "    image_widget.value = image_jpeg\n",
    "\n",
    "\n",
    "def motion_detection(firstFrame, newFrame, threshold=40):\n",
    "    \"\"\"\n",
    "    Detect motion by comparing the newFrame with a firstFrame.\n",
    "    \n",
    "    Arguments:\n",
    "        firstFrame (array): first frame considered as the baseline.\n",
    "        newFrame (array): new frame.\n",
    "        threshold (int): movement threshold.\n",
    "        \n",
    "    Returns:\n",
    "        list: of regions where movement has been detected (left, top, right, bottom)\n",
    "    \"\"\"\n",
    "    # Adapted from https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/\n",
    "\n",
    "    # Min movement area\n",
    "    MIN_AREA = 50\n",
    "\n",
    "    # Gaussian blur kernel\n",
    "    KERNEL_SIZE = 3\n",
    "\n",
    "    # List to store regions where movement is detected\n",
    "    motion_bboxes = []\n",
    "    \n",
    "    # firstFrame preprocessing\n",
    "    firstFrame = cv2.cvtColor(firstFrame, cv2.COLOR_BGR2GRAY)\n",
    "    firstFrame = cv2.GaussianBlur(firstFrame, (KERNEL_SIZE, KERNEL_SIZE), 0)\n",
    "    \n",
    "    # grab the current frame and initialize the static/moving text\n",
    "    frame = newFrame.copy()\n",
    "\n",
    "    # convert frame to grayscale, and blur it\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (KERNEL_SIZE, KERNEL_SIZE), 0)\n",
    "\n",
    "    # compute the absolute difference between the current frame and\n",
    "    # first frame\n",
    "    frameDelta = cv2.absdiff(firstFrame, gray)\n",
    "    thresh = cv2.threshold(frameDelta, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # dilate the thresholded image to fill in holes, then find contours\n",
    "    # on thresholded image\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)   \n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # if the contour is too small, ignore it\n",
    "        if cv2.contourArea(c) < MIN_AREA:\n",
    "            continue\n",
    "\n",
    "        # compute the bounding box for the contour, draw it on the frame,\n",
    "        # and update the text\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        \n",
    "        motion_bboxes.append((x, y, x+w, y+h))    # (left, top, right, bottom)\n",
    "    \n",
    "    return motion_bboxes\n",
    "\n",
    "def box_in_box(in_box, out_box, margin=0):\n",
    "    \"\"\"\n",
    "    Evaluate if a bounding box is partially inside another one.\n",
    "    \n",
    "    Arguments:\n",
    "        in_box (array): inner bounding box (left, top, right, bottom).\n",
    "        out_box (array): outter bounding box (left, top, right, bottom).\n",
    "        margin (int, optional): percentual margin to add to outter box. Defaults to 0.\n",
    "        \n",
    "    Returns:\n",
    "        boolean\n",
    "    \"\"\"\n",
    "    top_left = False\n",
    "    bottom_right = False\n",
    "    \n",
    "    # Apply margin\n",
    "    out_box[0] = int((1 - margin/100) * out_box[0])\n",
    "    out_box[1] = int((1 - margin/100) * out_box[1])\n",
    "    out_box[2] = int((1 + margin/100) * out_box[2])\n",
    "    out_box[3] = int((1 + margin/100) * out_box[3])\n",
    "    \n",
    "    # Top-left corner\n",
    "    if out_box[0] <= in_box[0] <= out_box[2] and out_box[1] <= in_box[1] <= out_box[3]:\n",
    "        top_left = True\n",
    "        \n",
    "    # Bottom-right corner\n",
    "    if out_box[0] <= in_box[2] <= out_box[2] and out_box[1] <= in_box[3] <= out_box[3]:\n",
    "        bottom_right = True\n",
    "            \n",
    "    # Return True if top-left and bottom-right corners are inside\n",
    "    if top_left and bottom_right:\n",
    "        return True\n",
    "    \n",
    "def allow_movement(jetbot_camera, image_widget, detection_time, video_writer=None):\n",
    "    \"\"\"\n",
    "    Perform person detection and display results.\n",
    "    \n",
    "    Arguments:\n",
    "        jetbot_camera(Camera): Jetbot camera initialized with size (300, 300).\n",
    "        image_widget (widgets.Image): Jupyter image widget.\n",
    "        video_writer (cv2.VideoWriter, optional): OpenCV video writer used to save output.\n",
    "        \n",
    "    \"\"\"\n",
    "    initial_t = time.time()\n",
    "    elapsed_time = 0\n",
    "\n",
    "    while elapsed_time < detection_time:\n",
    "        # Make people detections\n",
    "        img = jetbot_camera.value.copy()\n",
    "        people_detections = detect_people(model, img, 0.1)\n",
    "        \n",
    "        # Display image and detections\n",
    "        image = plot_boxes(img, people_detections)\n",
    "        display_image(image, image_widget, 'MOVE', f'{int(detection_time - elapsed_time)}s')\n",
    "        \n",
    "        # Save video\n",
    "        if video_writer:\n",
    "            video_writer.write(image)\n",
    "            \n",
    "        elapsed_time =  time.time() - initial_t\n",
    "\n",
    "    \n",
    "def detect_person_motion(jetbot_camera, image_widget, detection_time=5, threshold=40, video_writer=None):\n",
    "    \"\"\"\n",
    "    Perform person and motion detection in a detection window of detection_time\n",
    "    and display results.\n",
    "    \n",
    "    Arguments:\n",
    "        jetbot_camera(Camera): Jetbot camera initialized with size (300, 300).\n",
    "        image_widget (widgets.Image): Jupyter image widget.\n",
    "        detection_time(int, optional): Total to perform motion detection.\n",
    "        threshold (int): movement threshold.\n",
    "        video_writer (cv2.VideoWriter, optional): OpenCV video writer used to save output.\n",
    "    \"\"\"    \n",
    "    # Get first frame\n",
    "    first_frame = jetbot_camera.value.copy()\n",
    "\n",
    "    initial_t = time.time()\n",
    "    elapsed_time = 0\n",
    "    t = \"\"\n",
    "\n",
    "    # Make people detections\n",
    "    people_detections = detect_people(model, jetbot_camera.value.copy(), 0.1)\n",
    "    \n",
    "    # Run motion detection for MAX_TIME\n",
    "    moving_persons_total = []\n",
    "    while elapsed_time < detection_time:\n",
    "        # Get new frame\n",
    "        img = jetbot_camera.value.copy()\n",
    "        \n",
    "        # Detect motion\n",
    "        m_bboxes = motion_detection(first_frame, img, threshold)        \n",
    "        m_bboxes = set(m_bboxes)    # remove duplicates\n",
    "        \n",
    "        # Detect which person is moving\n",
    "        moving_persons = []\n",
    "        for i in range(len(people_detections)):\n",
    "            out_box = people_detections[i]                \n",
    "            person_num = i+1\n",
    "            \n",
    "            for in_box in m_bboxes:\n",
    "                if box_in_box(list(in_box), out_box):\n",
    "                    if person_num not in moving_persons:\n",
    "                        moving_persons.append(person_num)\n",
    "                        \n",
    "        moving_persons_total.extend(moving_persons)\n",
    "        \n",
    "        # Persons moved in this frame\n",
    "        if len(moving_persons):\n",
    "            str_nums = \", \".join([str(i) for i in moving_persons])\n",
    "            t = f'Moving: {str_nums}'\n",
    "        \n",
    "        # Display image and detections\n",
    "        image = plot_boxes(img, people_detections, m_bboxes)\n",
    "        display_image(image, image_widget, 'STOP', t)\n",
    "        \n",
    "        # Save video\n",
    "        if video_writer:\n",
    "            video_writer.write(image)\n",
    "            \n",
    "        elapsed_time =  time.time() - initial_t\n",
    "\n",
    "    # Print persons moved in detection window\n",
    "    moving_persons_total = set(moving_persons_total)\n",
    "    if len(moving_persons_total):\n",
    "        str_nums = \", \".join([str(i) for i in moving_persons_total])\n",
    "        print(f'Person(s) moving: {str_nums}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='400', width='400')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image widget\n",
    "image_widget = widgets.Image(format='jpeg', width=400, height=400)\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Person(s) moving: 1\n",
      "Person(s) moving: 1\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "stop_time = 3\n",
    "move_time = 6\n",
    "\n",
    "# Video Writer\n",
    "cv2_video_writer = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (300,300))\n",
    "# cv2_video_writer = None\n",
    "\n",
    "# Main loop\n",
    "print('Start')\n",
    "while True:\n",
    "    try:\n",
    "        # MOVE - Allow movement\n",
    "        allow_movement(camera, image_widget, move_time, cv2_video_writer)\n",
    "\n",
    "        # STOP - Detect movement\n",
    "        detect_person_motion(camera, image_widget, stop_time, video_writer=cv2_video_writer)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print('Finish')        \n",
    "cv2_video_writer.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
